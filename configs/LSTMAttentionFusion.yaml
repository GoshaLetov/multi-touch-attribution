experiment_name: 'LSTMAttentionFusion'

train:
  monitor_metric: 'valid.AUC'
  monitor_mode: 'max'
  accelerator: 'gpu'
  device: 0
  n_epochs: 30
  seed: 42
  lr: 0.05

model:
  backbone: 'LSTMAttentionFusion'
  num_embeddings: 7
  embedding_dim: 32
  hidden_size: 64
  num_layers: 3
  dropout: 0.05
  non_linearity: 'identity'
  time_decay: 0.05
  controls: true

data:
  path: '/content/drive/MyDrive/rk_events_channels_20_000.csv'
  path_controls: '/content/drive/MyDrive/rk_features_channels_20_000.csv'
  num_workers: 2
  train_fraction: 0.8
  train_batch_size: 64
  valid_batch_size: 64
